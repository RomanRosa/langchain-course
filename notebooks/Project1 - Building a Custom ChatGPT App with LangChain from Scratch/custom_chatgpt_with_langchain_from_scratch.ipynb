{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Custome ChatGPT App With LangChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import MessagesPlaceholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Chat Memory using ConversationBufferMemory\n",
    "memory= ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The softmax activation function is commonly used in machine learning and deep learning models, particularly in multi-class classification problems. It takes a vector of real numbers as input and produces a vector of probabilities as output.\n",
      "\n",
      "The math behind the softmax function involves exponentiating each element of the input vector and then normalizing the resulting values. Here is the formula for the softmax function:\n",
      "\n",
      "softmax(x) = exp(x_i) / sum(exp(x_j)) for all j\n",
      "\n",
      "In this formula, x represents the input vector, and i and j represent the indices of the elements within the vector.\n",
      "\n",
      "The exponentiation step makes the elements positive and amplifies larger values, while the normalization step ensures that the output vector sums up to 1, representing a valid probability distribution.\n",
      "\n",
      "The softmax function is often used in the last layer of a neural network model for classification tasks. It assigns a probability to each class and selects the class with the highest probability as the predicted output. By using softmax, the model can handle multiple classes and produce probabilities that sum up to 1, making the interpretation and decision-making process easier.\n",
      "--------------------------------------------------\n",
      "The softmax activation function is commonly used in the output layer of a neural network for multi-class classification tasks. It transforms the raw predicted values into a probability distribution over the classes.\n",
      "\n",
      "When a neural network produces a set of raw outputs (also called logits), the softmax function computes the exponentiated values of each logit and normalizes them by dividing each value by the sum of all exponentiated logits. This normalization ensures that the resulting values lie between 0 and 1 and add up to 1, representing probabilities.\n",
      "\n",
      "The softmax function is mathematically defined as follows:\n",
      "\n",
      "softmax(x_i) = exp(x_i) / sum(exp(x_j)) for all j\n",
      "\n",
      "where x_i is the i-th logit and the sum is taken over all logit values.\n",
      "\n",
      "The softmax function effectively transforms the raw values into probabilities that indicate the likelihood of an input belonging to each class. The class with the highest probability is typically chosen as the predicted output.\n",
      "\n",
      "Apart from multi-class classification problems, the softmax function is also used in other contexts, such as language modeling, where it helps generate probability distributions over the next word in a sequence given the previous words.\n",
      "--------------------------------------------------\n",
      "The Rectified Linear Unit (ReLU) activation function is one of the most widely used activation functions in deep learning models. It introduces non-linearity into the network while being computationally efficient.\n",
      "\n",
      "The ReLU function is defined as follows:\n",
      "\n",
      "ReLU(x) = max(0, x)\n",
      "\n",
      "Simply put, for any input value x, ReLU outputs the maximum of zero and x. If the input x is positive, ReLU returns the input value as it is. If the input x is negative, ReLU returns zero. Therefore, ReLU effectively \"turns on\" for positive values and \"turns off\" for negative values.\n",
      "\n",
      "There are a few key characteristics and advantages of the ReLU activation function:\n",
      "\n",
      "1. Simplicity: ReLU is a simple and computationally efficient activation function compared to other complex functions, such as the sigmoid or tanh.\n",
      "\n",
      "2. Non-linearity: ReLU introduces non-linearity into the network, which helps the model learn and represent complex patterns and relationships in the data.\n",
      "\n",
      "3. Sparse activation: ReLU can lead to sparsity in the network since it only activates a subset of neurons that receive positive inputs.\n",
      "\n",
      "4. Addressing the vanishing gradient problem: ReLU helps alleviate the vanishing gradient problem, often encountered in deep neural networks, by avoiding saturation for positive inputs.\n",
      "\n",
      "Although ReLU works well for most scenarios, one possible drawback is that neurons with negative values do not contribute to the network's learning, leading to the problem of \"dead neurons.\" To mitigate this, variations of ReLU, such as Leaky ReLU and Parametric ReLU, have been proposed. These variants introduce a small slope or allow the neuron to learn the slope for negative inputs, thus addressing the dead neuron issue.\n",
      "\n",
      "Overall, ReLU is a powerful activation function that has proven to be effective in many deep learning applications and is widely used in various network architectures.\n",
      "--------------------------------------------------\n",
      "Earth mass refers to the mass of the Earth, which is a measure of the amount of matter contained within the Earth. It is commonly used in scientific calculations and discussions related to celestial bodies, astronomy, and planetary science.\n",
      "\n",
      "The mass of the Earth is approximately 5.972 × 10^24 kilograms (kg) or about 1.316 × 10^25 pounds (lb). This value represents the total mass of the Earth, which includes its core, mantle, crust, and atmosphere.\n",
      "\n",
      "Understanding the Earth's mass is crucial for various scientific studies, including gravity calculations, orbital mechanics, and the Earth's interactions with other celestial objects. It serves as a fundamental parameter for many equations and theories, providing valuable insights into the structure, dynamics, and properties of our planet.\n",
      "--------------------------------------------------\n",
      "The diameter of the Earth is a measurement of the distance from one side of the Earth to the opposite side, passing through its center. It is commonly used to describe the size and scale of our planet.\n",
      "\n",
      "The approximate mean diameter of the Earth is about 12,742 kilometers (7,918 miles). This measurement is based on a mean value, as the Earth is not a perfect sphere and has some irregularities in its shape.\n",
      "\n",
      "The Earth's diameter is an essential parameter in various scientific calculations, such as determining the Earth's volume, surface area, and gravitational interactions. It is also used as a reference point for comparing the sizes of other celestial bodies and understanding the scale of the universe.\n",
      "--------------------------------------------------\n",
      "To multiply 4 by 20, you simply multiply the two numbers together. \n",
      "\n",
      "4 * 20 = 80\n",
      "--------------------------------------------------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "llm= ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "prompt= ChatPromptTemplate(\n",
    "    input_variables=['content'],\n",
    "    messages=[\n",
    "        SystemMessage(content='You are a chatbot having a conversation with a human!.'),\n",
    "        MessagesPlaceholder(variable_name='chat_history'), # Where The Memory Will Be Stored\n",
    "        HumanMessagePromptTemplate.from_template('{content}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain= LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "while True:\n",
    "    content= input('Your Prompt: ')\n",
    "    if content in ['quit','exit','bye']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "    \n",
    "    response= chain.run({'content':content})\n",
    "    print(response)\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
