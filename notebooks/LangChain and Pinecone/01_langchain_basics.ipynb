{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LangChain Basics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.0.352\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: c:\\anaconda3\\envs\\langchain\\lib\\site-packages\n",
      "Requires: aiohttp, async-timeout, dataclasses-json, jsonpatch, langchain-community, langchain-core, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: langchain-experimental\n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Python-dotenv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LLM Models (Wrappers): GPT-3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'text-davinci-003', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 512}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm= OpenAI(model_name='text-davinci-003', temperature=0.7, max_tokens=512)\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Quantum mechanics is a physical theory that describes the behavior of matter and energy at the smallest scales of atoms and subatomic particles.\n"
     ]
    }
   ],
   "source": [
    "output= llm('Explain Quantum Mechanics in One Sentence')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(llm.get_num_tokens('Explain Quantum Mechanics in One Sentence'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output= llm.generate(['...is the capital of France.',\n",
    "                      'What is the formula for the area of a circle?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Generation(text='\\n\\nParis', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nArea = πr^2, where r is the radius of the circle.', generation_info={'finish_reason': 'stop', 'logprobs': None})]]\n"
     ]
    }
   ],
   "source": [
    "print(output.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Area = πr^2, where r is the radius of the circle.\n"
     ]
    }
   ],
   "source": [
    "print(output.generations[1][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Paris\n"
     ]
    }
   ],
   "source": [
    "print(output.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output= llm.generate(['Write an original tagline for a burger restaurant']*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='\\n\\n\"Taste the deliciousness that only a Burger from [restaurant name] can provide!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Sink your teeth into something delicious - The Burger House!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"The Best Burgers in Town, Guaranteed!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'total_tokens': 77, 'prompt_tokens': 27, 'completion_tokens': 50}, 'model_name': 'text-davinci-003'} run=[RunInfo(run_id=UUID('48f90a54-8b5a-4bda-bd11-10510e7518c4')), RunInfo(run_id=UUID('0935ff4b-d837-43ff-8f7a-b377471da316')), RunInfo(run_id=UUID('cb762ee0-0d00-494a-b535-948f9da78f48'))]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Taste the deliciousness that only a Burger from [restaurant name] can provide!\"\n",
      "\n",
      "\"Sink your teeth into something delicious - The Burger House!\"\n",
      "\n",
      "\"The Best Burgers in Town, Guaranteed!\""
     ]
    }
   ],
   "source": [
    "for o in output.generations:\n",
    "    print(o[0].text, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ChatModels: GPT-3.5-Turbo and GPT-4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat= ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5, max_tokens=1024)\n",
    "messages= [\n",
    "    SystemMessage(content='You are a physicist and respond only in German.'),\n",
    "    HumanMessage(content='Explain Quantum Mechanics in One Sentence')\n",
    "]\n",
    "output=chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Quantenmechanik beschreibt das Verhalten von Teilchen und Wellen auf subatomarer Ebene und ermöglicht uns, ihre Eigenschaften und Wechselwirkungen zu verstehen.'\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantenmechanik beschreibt das Verhalten von Teilchen und Wellen auf subatomarer Ebene und ermöglicht uns, ihre Eigenschaften und Wechselwirkungen zu verstehen.\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prompt Templates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language', 'virus'] template='You are an experienced viologist.\\nWrite a few sentences about the following {virus} in {language}'\n"
     ]
    }
   ],
   "source": [
    "template='''You are an experienced viologist.\n",
    "Write a few sentences about the following {virus} in {language}'''\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    input_variables=['virus','language'],\n",
    "    template=template\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "HIV ist eine der häufigsten und gefährlichsten sexuell übertragbaren Krankheiten der Welt. Es wird durch ein Virus übertragen, das das Immunsystem schädigt, und es gibt keine Heilung. HIV kann durch ungeschützten Geschlechtsverkehr, durch Blutkontakt oder durch den Austausch von Nadeln übertragen werden. Es ist wichtig, dass wir uns alle schützen, indem wir sichere Sexualpraktiken anwenden, Blutkonserven sorgfältig überprüfen und Nadeln nicht weitergeben.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm=OpenAI(model_name='text-davinci-003', temperature=0.7)\n",
    "output=llm(prompt.format(virus='HIV', language='German'))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Simple Chains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize the language model with the specified model and temperature\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "\n",
    "# Define a prompt template with variables for virus and language\n",
    "template = '''You are an experienced virologist.\n",
    "Write a few sentences about the following virus {virus} in language {language}.'''\n",
    "\n",
    "# Create a prompt template object with specified input variables and template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['virus', 'language'],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Create a chain with the language model and prompt template\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Execute the chain with specified input parameters and obtain the output\n",
    "output = chain.run({'virus': 'HSV', 'language': 'French'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'HSV, ou le virus de l'herpès simplex, est un virus courant qui infecte les humains. Il existe deux types d'HSV : le HSV-1, qui provoque généralement des lésions buccales et labiales, et le HSV-2, qui est principalement responsable des infections génitales. Ces virus sont hautement contagieux et se propagent par contact direct avec les lésions ou les sécrétions infectées. Bien qu'il n'existe pas de traitement curatif pour l'HSV, des médicaments antiviraux peuvent aider à réduire la gravité et la fréquence des poussées. Il est important de prendre des précautions pour éviter la transmission de l'HSV, notamment en utilisant des préservatifs et en évitant les contacts étroits lors d'une poussée active.\n"
     ]
    }
   ],
   "source": [
    "# Print the output\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sequential Chains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm1=OpenAI(model_name='text-davinci-003', temperature=0.7, max_tokens=1024)\n",
    "prompt1=PromptTemplate(\n",
    "    input_variables=['concept'],\n",
    "    template='''You are an experienced scientist and python programmer.\n",
    "    Write a function that implements the concept of {concept}.'''\n",
    ")\n",
    "chain1=LLMChain(llm=llm1, prompt=prompt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2= ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1.2)\n",
    "prompt2=PromptTemplate(\n",
    "    input_variables=['function'],\n",
    "    template='Given the python {function}, describe it as detailed as possible.'\n",
    ")\n",
    "chain2=LLMChain(llm=llm2, prompt=prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "def linear_regression(x, y):\n",
      "    # Calculate linear regression parameters\n",
      "    n = len(x)\n",
      "    sum_x = sum(x)\n",
      "    sum_y = sum(y)\n",
      "    sum_xy = sum([x[i]*y[i] for i in range(n)])\n",
      "    sum_x2 = sum([x[i]**2 for i in range(n)])\n",
      "    \n",
      "    # Calculate linear regression coefficients\n",
      "    b1 = (n*sum_xy - sum_x*sum_y) / (n*sum_x2 - sum_x**2)\n",
      "    b0 = (sum_y - b1*sum_x) / n\n",
      "    \n",
      "    # Return linear regression coefficients\n",
      "    return b0, b1\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThis function performs simple linear regression given two lists, x and y, which represent input variables and corresponding output variables, respectively.\n",
      "\n",
      "1. First, the function calculates the length of the lists, x and y, and assigns it to the variable \"n\".\n",
      "\n",
      "2. The function calculates the sum of all elements in the list x using the \"sum\" function and assigns it to the variable \"sum_x\".\n",
      "\n",
      "3. The function calculates the sum of all elements in the list y using the \"sum\" function and assigns it to the variable \"sum_y\".\n",
      "\n",
      "4. The function uses a list comprehension to iterate over the indices of the lists x and y simultaneously. It multiples each pair of corresponding elements and calculates the sum of the resulting products. The sum is assigned to the variable \"sum_xy\".\n",
      "\n",
      "5. The function uses another list comprehension to iterate over the indices of the list x and squares each corresponding element. The function calculates the sum of all the squared elements and assigns it to the variable \"sum_x2\".\n",
      "\n",
      "6. The function calculates the slope of the regression line (b1) using the formula: (n*sum_xy - sum_x*sum_y) / (n*sum_x2 - sum_x**2).\n",
      "\n",
      "7. The function calculates the y-intercept of the regression line (b0) using the formula: (sum_y - b1*sum_x) / n.\n",
      "\n",
      "8. Finally, the function returns the calculated values of b0 and b1 as a tuple.\n",
      "\n",
      "In essence, this function calculates the least squares estimate of the coefficients for a simple linear regression model, which can be used to predict the value of the output variable (y) given a value for the input variable (x).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "overall_chain=SimpleSequentialChain(chains=[chain1,chain2], verbose=True)\n",
    "output=overall_chain.run('linear regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "def softmax(x):\n",
      "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
      "    e_x = np.exp(x - np.max(x))\n",
      "    return e_x / e_x.sum()\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output=overall_chain.run('softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "def relu(x):\n",
      "    if x >= 0:\n",
      "        return x\n",
      "    else:\n",
      "        return 0\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe given Python code defines a function named `relu` that implements the Rectified Linear Unit (ReLU) activation function. ReLU is a commonly used activation function in neural networks when no negative output values are desired. The function takes a single argument `x`, representing an input value.\n",
      "\n",
      "Here is the detailed explanation of the code:\n",
      "1. The `relu` function definition begins with the keyword `def`. The function name is `relu`, and it requires one parameter `x`. This parameter represents the input value to be processed.\n",
      "2. The body of the function is indented below the function definition and contains the logic to compute the ReLU activation.\n",
      "3. ReLU activation has a simple condition for transformation:\n",
      "  - If the input value `x` is greater than or equal to 0, the function returns the same value.\n",
      "  - If the input value `x` is negative, the function returns 0 instead.\n",
      "4. The `if` statement is used to check the condition `x >= 0`. If the condition is true, it executes the indented code block below it, which consists only of a return statement.\n",
      "5. The return statement utilizes the Python `return` keyword to exit the function and provide a result as the output of the function.\n",
      "6. If the condition in the if statement evaluates to false, the code jumps to the `else` block.\n",
      "7. The `else` block consists of a single return statement that returns 0 when the input `x` is negative.\n",
      "8. When calling this `relu` function with an input value, the function first checks if the input value is positive or zero. If it is, the input value is directly returned. Otherwise, 0 is returned.\n",
      "\n",
      "Overall, this code implements the ReLU activation function by returning the input value if it is non-negative (greater than or equal to zero) and returning 0 otherwise.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output=overall_chain.run('relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LangChain Agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonAstREPLTool\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(temperature=0)\n",
    "agent_executor=create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonAstREPLTool(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I can use the math library to calculate the square root and factorial of a number.\n",
      "Action: python_repl_ast\n",
      "Action Input: import math\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Now that I have imported the math library, I can use the sqrt and factorial functions.\n",
      "Action: python_repl_ast\n",
      "Action Input: round(math.sqrt(math.factorial(20)), 4)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m1559776268.6285\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to round the result to 4 decimal points.\n",
      "Action: python_repl_ast\n",
      "Action Input: round(1559776268.6285, 4)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m1559776268.6285\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m The result is still not rounded to 4 decimal points, I need to use the format function.\n",
      "Action: python_repl_ast\n",
      "Action Input: format(1559776268.6285, '.4f')\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m1559776268.6285\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Now the result is rounded to 4 decimal points.\n",
      "Final Answer: 1559776268.6285\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1559776268.6285'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run('Calculate the square root of the factorial of 20\\\n",
    "                   and display it with 4 decimal points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I can use the power operator to raise 5.1 to the power of 7.3\n",
      "Action: python_repl_ast\n",
      "Action Input: 5.1**7.3\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m146306.05007233328\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 146306.05007233328\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'146306.05007233328'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run('what is the answer to 5.1**7.3?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
